import os
from copy import copy
from utils import configure_logger, clean
import tensorflow as tf
import pandas as pd
import numpy as np
import pickle
from nltk.tokenize import RegexpTokenizer
w_tokenizer = RegexpTokenizer('\w+')


sequence_length = 100
lstm_num_units = 256

dic = pickle.load(open('../models/dictionary', 'rb'))
stored_embeddings = pickle.load(open('../models/embeddings', 'rb'))


def create_data(chosen_author, split = [0.7,0.85], min_words = sequence_length):
    """
    Input:
        min_words: min words in article for it to be in data
        num_refs: number of texts to use as references to compare to candidate text
    Output: CSV file containing filenames of texts to serve as features and labels corresponding to whether a candidate is from the same author
    """

    parent_dir = '../data/Reuters-50'
    authors = os.listdir(parent_dir)
    count = len(authors)
    c = 0

    # Discard texts shorter than min length. Do first seperately so we don't have to check length again when choosing candidates from other authors
    data = pd.DataFrame({}, columns = ['author', 'file', 'pos', 'words', 'target'])

    for author in authors:
        texts = os.listdir(os.path.join(parent_dir, author))
        if author != chosen_author:
            texts = np.random.choice(texts, 2, replace=False)
        for text in texts:
            with open(os.path.join(parent_dir,author,text)) as f:
                content = f.read()
                cleaned_content = clean(content)
                words = w_tokenizer.tokenize(cleaned_content)
                wc = len(words)
                if wc > min_words:
                    num_samples = wc // min_words
                    for pos in range(num_samples):
                        idx = list(map(lambda x: dic.get(x, 0), words[pos*min_words : (pos+1)*min_words]))
                        if author == chosen_author:
                            target = 1
                        else:
                            target = 0
                        row = [chosen_author, os.path.join(author, text), pos, idx, target]
                        data = data.append(dict(zip(data.columns, row)), ignore_index=True)

    data = data.sample(frac=1)
    valid_split = int(split[0]*len(data))
    test_split = int(split[1]*len(data))
    train_data = data.iloc[:valid_split,:]
    valid_data = data.iloc[valid_split:test_split,:]
    test_data = data.iloc[test_split:,:]

    return train_data, valid_data, test_data


def generate_batch(data, batch_num, size):
    subset = data.iloc[batch_num*size : batch_num*size + size,:]
    candidates = np.stack(subset.loc[:,'words'].values)
    targets = subset.iloc[:,-1].values.reshape(size, 1)
    return candidates, targets


def get_accuracy(outs, labels):
    sigmoids = tf.sigmoid(outs)
    preds = tf.round(sigmoids)
    score = tf.equal(preds, labels)
    accuracy = tf.reduce_mean(tf.cast(score, tf.float32))
    return accuracy


def feed_forward(cand_embed):
    with tf.variable_scope('',reuse=tf.AUTO_REUSE):
        initializer = tf.initializers.truncated_normal()
        lstm_cell = tf.contrib.rnn.BasicLSTMCell(lstm_num_units, activation = tf.nn.tanh)
        LSTM_candidates_outs = tf.nn.dynamic_rnn(lstm_cell, cand_embed, dtype = tf.float32)
        print(LSTM_candidates_outs[1].h)
        last_states_candidates = LSTM_candidates_outs[1].h
        layer1 = tf.layers.dense(last_states_candidates, 128, kernel_initializer = initializer, activation = tf.nn.relu)
        layer2 = tf.layers.dense(layer1, 64, kernel_initializer = initializer, activation = tf.nn.relu)
        outputs = tf.layers.dense(layer2, 1, kernel_initializer = initializer, activation = None)
    return outputs


def train(train_data, valid_data, test_data, epochs = 10, batch_size = 64):
    logger = configure_logger(modelname = 'lstm_per_author_train')

    graph = tf.Graph()
    with graph.as_default():
        embeddings = tf.constant(stored_embeddings, dtype = tf.float32)

        train_candidates = tf.placeholder(tf.int32, shape = (None, sequence_length))
        train_targets = tf.placeholder(tf.float32, shape = (None,1))
        train_candidates_embed = tf.nn.embedding_lookup(embeddings, train_candidates)

        valid_candidates = tf.constant(generate_batch(valid_data, 0, len(valid_data))[0], dtype = tf.int32)
        valid_targets = tf.constant(generate_batch(valid_data, 0, len(valid_data))[1], dtype = tf.float32)
        valid_candidates_embed = tf.nn.embedding_lookup(embeddings, valid_candidates)

        test_candidates = tf.constant(generate_batch(test_data, 0, len(valid_data))[0], dtype = tf.int32)
        test_targets = tf.constant(generate_batch(test_data, 0, len(valid_data))[1], dtype = tf.float32)
        test_candidates_embed = tf.nn.embedding_lookup(embeddings, test_candidates)

        all_train_candidates = tf.constant(generate_batch(train_data, 0, len(train_data))[0], dtype = tf.int32)
        all_train_targets = tf.constant(generate_batch(train_data, 0, len(train_data))[1], dtype = tf.float32)
        all_train_candidates_embed = tf.nn.embedding_lookup(embeddings, all_train_candidates)

        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=train_targets, logits=feed_forward(train_candidates_embed)))
        optimizer = tf.train.AdamOptimizer().minimize(loss)

        all_train_preds = tf.round(tf.sigmoid(feed_forward(all_train_candidates_embed)))
        all_train_accuracy = get_accuracy(all_train_preds, all_train_targets)

        valid_preds = tf.round(tf.sigmoid(feed_forward(valid_candidates_embed)))
        valid_accuracy = get_accuracy(valid_preds, valid_targets)

        test_preds = tf.round(tf.sigmoid(feed_forward(test_candidates_embed)))
        test_accuracy = get_accuracy(test_preds, test_targets)

        saver = tf.train.Saver()


    num_batches = len(train_data) // batch_size
    with tf.Session(graph = graph) as sess:
        tf.global_variables_initializer().run()
        cum_loss = 0
        for epoch in range(epochs):
            train_data = train_data.sample(frac=1)
            for batch in range(num_batches):
                batch_candidates, batch_targets = generate_batch(train_data, batch, batch_size)
                feed_dict = {train_candidates: batch_candidates, train_targets: batch_targets}
                _, l = sess.run([optimizer, loss], feed_dict=feed_dict)
                msg = 'Batch {} of {}. Loss: {:0.3f}'.format(batch + 1, num_batches, l)
                print(msg)
                logger.info(msg)
            preds = all_train_preds.eval()
            targets = all_train_targets.eval()
            np.set_printoptions(threshold=np.nan)
            logger.info(np.concatenate([preds, targets], axis = 1))
            msg = 'Done epoch {}. Accuracy on training set: {:.1%}'.format(epoch, all_train_accuracy.eval())
            print(msg)
            logger.info(msg)
        saver.save(sess, '../models/lstm_per_author/model')
        logger.info('Training finished. Saved model')


def run_model(author):
    train_data, valid_data, test_data = create_data(author, split = [0.7, 0.85])
    output = train(train_data, valid_data, test_data, epochs = 10)


if __name__ == "__main__":
    train_data, valid_data, test_data = create_data('AaronPressman', split = [0.7, 0.85])
    train(train_data, valid_data, test_data, epochs = 20)
